import json
import logging
import time
from typing import Any, Generator

import openai
import tiktoken
from ava.clients.sql.crud import select_settings_from_key
from ava.clients.sql.schema import Settings_Schema
from ava.model.dscom import User
from ava.model.ModelTokenLogEntry import ModelTokenLogEntry
from ava.utils.BaseDocumentsAgent import BaseDocumentsAgent
from ava.utils.return_message_style import error_msg
from ava.utils.utils import LoggerUtils
from kor.extraction import create_extraction_chain
from langchain_community.callbacks import get_openai_callback
from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam
from retrying import retry

logger = logging.getLogger("ava_app")
token_logger = logging.getLogger("token_counter")


# GPT ç›¸é—œä»»å‹™
class GPTHandler(BaseDocumentsAgent):

    def __init__(self, api_key, base_url: str | None = None, params=None):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.params = params if params is not None else {}

    # paramså¯ä»¥åƒè€ƒ Create chat completion: https://platform.openai.com/docs/api-reference/chat/create?lang=python
    def run_gptModel(
        self,
        session,
        user: User,
        messages,
        user_input,
        expert_data,
        chat_uuid,
        model_list_id: int,
        classify="non-streaming",
        model="gpt-4o",
        params=None,
    ):
        if params is None:
            params = self.params
        try:
            params["stream"] = False
            response = self.client.chat.completions.create(model=model,
                                                           messages=messages,
                                                           timeout=50,
                                                           **params)
            content = response.choices[0].message.content
            logger.debug(f"response.choices[0] {response.choices[0]}")

            # å»ºç«‹ ModelTokenLogEntry å¯¦ä¾‹
            entry = ModelTokenLogEntry(
                model_list_id=model_list_id,
                users_id=user.uid,
                model=model,
                classify=classify,
                prompt_token=response.usage.prompt_tokens,
                completion_token=response.usage.completion_tokens,
                user_input=user_input,
                expert_id=expert_data["expert_id"],
                expert_model=expert_data["expert_model"],
                expert_model_type=expert_data["expert_model_type"],
                chat_uuid=chat_uuid,
            )
            LoggerUtils.log_token_info(entry, session)
            # LoggerUtils.log_token_info(
            #     user,
            #     model,
            #     response.usage.prompt_tokens,
            #     response.usage.completion_tokens,
            #     "-1",
            #     "intent",
            # )
            return content
        except Exception as e:
            logger.error(f"Error in GPTHandler get_gptModel: {e}")
            logger.debug(f"user {user}")
            logger.debug(f"model {model}")
            logger.debug(f"messages {messages}")
            logger.debug(f"params {params}")
            logger.debug(f"user_input {user_input}")
            logger.debug(f"expert_data {expert_data}")
            logger.debug(f"chat_uuid {chat_uuid}")
            return None

    def run_gptModel_stream(
        self,
        session,
        chat_uuid,
        user: User,
        messages,
        user_input,
        expert_data,
        model_list_id: int,
        classify="streaming",
        model="gpt-4o",
        params=None,
        history_message_id=None,
    ):
        if params is None:
            params = self.params
        try:
            params["stream"] = True
            logger.debug(f"é€å‡ºçš„ prompt : {messages}")
            t0 = time.time()
            response_stream = self.client.chat.completions.create(
                model=model,
                messages=messages,
                stream_options={"include_usage": True},
                timeout=50,
                **params)
            t1 = time.time()
            logger.info(
                f"ğŸ“¡ OpenAI chat completion stream created: {t1 - t0:.2f}s")
            # full_response = []
            for i, chunk in enumerate(response_stream):
                if i == 0:
                    first_token_time = time.time()
                    logger.info(
                        f"â±ï¸ OpenAI first token received: {first_token_time - t1:.2f}s after iteration start"
                    )
                if hasattr(chunk,
                           'choices') and len(chunk.choices) > 0 and hasattr(
                               chunk.choices[0], 'finish_reason'):
                    if chunk.choices[0].finish_reason == "length":
                        error_message = (
                            " \r\n\n " +
                            error_msg("å›æ‡‰é•·åº¦è¶…éé•·åº¦é™åˆ¶ï¼Œç„¡æ³•ç¹¼çºŒï¼Œè«‹å…·é«”æè¿°æ‚¨çš„å•é¡Œï¼Œè®“avaæ›´å¥½çš„ç‚ºæ‚¨æœå‹™ã€‚")
                        )
                        logger.info(f"Length limit reached: {error_message}")
                        yield error_message
                    elif hasattr(chunk.choices[0], "delta") and hasattr(
                            chunk.choices[0].delta, "content"):
                        content = chunk.choices[0].delta.content
                        if isinstance(content, str):
                            # full_response.append(content)
                            yield content
                        else:
                            logger.error(f"Content is not a string: {content}")
                elif hasattr(chunk, 'usage'):
                    if hasattr(chunk.usage, 'completion_tokens') and hasattr(
                            chunk.usage, 'prompt_tokens'):
                        completion_tokens = chunk.usage.completion_tokens
                        prompt_tokens = chunk.usage.prompt_tokens
                        logger.debug(
                            f"Completion Tokens: {completion_tokens}, Prompt Tokens: {prompt_tokens}"
                        )
                    else:
                        logger.error(
                            "Chunk usage does not have completion_tokens or prompt_tokens"
                        )
                        raise Exception("å‡ºç¾éŒ¯èª¤äº†ï¼Œå›æ‡‰å…§å®¹ä¸­æœ‰è³‡æ–™ç¼ºå¤±ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡")
                else:
                    logger.error("Chunk does not have valid choices or usage")
                    raise Exception("å‡ºç¾éŒ¯èª¤äº†ï¼Œå›æ‡‰å…§å®¹ä¸­æœ‰è³‡æ–™ç¼ºå¤±ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡")
            entry = ModelTokenLogEntry(
                model_list_id=model_list_id,
                users_id=user.uid,
                model=model,
                user_input=user_input,
                classify=classify,
                expert_id=expert_data["expert_id"],
                expert_model=expert_data["expert_model"],
                expert_model_type=expert_data["expert_model_type"],
                chat_uuid=chat_uuid,
                prompt_token=prompt_tokens,
                completion_token=completion_tokens,
                history_message_id=history_message_id,
            )
            LoggerUtils.log_token_info(entry, session)
            # full_body = "".join(full_response)
            # prompt_text = (" ".join(
            #     json.dumps(msg) if isinstance(msg, dict) else msg
            #     for msg in messages)
            #                if isinstance(messages, list) else messages)
            # entry = ModelTokenLogEntry(
            #     model_list_id=model_list_id,
            #     users_id=user.uid,
            #     model=model,
            #     user_input=user_input,
            #     classify=classify,
            #     expert_id=expert_data["expert_id"],
            #     expert_model=expert_data["expert_model"],
            #     expert_model_type=expert_data["expert_model_type"],
            #     chat_uuid=chat_uuid,
            # )
            # LoggerUtils.token_from_text(prompt_text, full_body, entry, session)
        except openai.APIConnectionError as e:
            # Handle connection error here
            logger.error(f"Failed to connect to OpenAI API: {e}")
            yield error_msg("å‡ºç¾éŒ¯èª¤äº†ï¼Œé€£ç·šopenAIå¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦")
        except openai.BadRequestError as e:
            # Handle API error here, e.g. retry or log
            logger.error(f"OpenAI API BadRequestError an API Error: {e}")
            yield error_msg("å‡ºç¾éŒ¯èª¤äº†ï¼Œå¯èƒ½æ˜¯è¶…å‡ºtokené•·åº¦ï¼Œè«‹è©¦è‘—ç¸®å°å•é¡Œçš„ç¯„åœæˆ–æ”¹è®Šå•ç­”æ–¹å¼")

        except openai.APIError as e:
            # Handle API error here, e.g. retry or log
            logger.error(f"OpenAI API returned an API Error: {e}")
            yield error_msg("éŒ¯èª¤ä»£ç¢¼:506 openAIå‡ºç¾éŒ¯èª¤äº†ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡")
        except Exception as e:
            logger.error(f"Error in GPTHandler run_gptModel_stream: {e}")
            yield error_msg("æ„å¤–éŒ¯èª¤ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡")

    @staticmethod
    def run_gptModel_langChain(session, chat_uuid, user, llm, expert_data,
                               schema, query, model_list_id):
        with get_openai_callback() as cb:
            chain = create_extraction_chain(llm, schema)

            data = chain.run(query)["data"]
            entry = ModelTokenLogEntry(
                model_list_id=model_list_id,
                users_id=user.uid,
                model=llm.model_name,
                prompt_token=cb.prompt_tokens,
                completion_token=cb.completion_tokens,
                user_input=query,
                expert_id=expert_data["expert_id"],
                expert_model=expert_data["expert_model"],
                expert_model_type=expert_data["expert_model_type"],
                chat_uuid=chat_uuid,
            )

            LoggerUtils.token_langchain(entry, session)
        return data

    def run_gptModel_for_voice(
        self,
        user: User,
        transcribed_text,
        model="gpt-4o",
        params=None,
    ):
        if params is None:
            params = self.params
        try:
            # å°‡èªéŸ³è½‰æ›çš„æ–‡æœ¬ä½œç‚ºè¼¸å…¥
            response = self.client.chat.completions.create(model=model,
                                                           messages=[{
                                                               "role":
                                                               "system",
                                                               "content":
                                                               transcribed_text
                                                           }],
                                                           **params)
            content = response.choices[0].message.content
            logger.debug(f"GPT Response: {content}")
            return content
        except Exception as e:
            logger.error(f"Error in run_gptModel_for_voice: {e}")
            return None

    def generate_model_response(
            self, *, session, chat_uuid: str, user: User, user_input: str,
            expert_data: dict[str, Any], model_list_id: int, model_name: str,
            history_message_id: str, file_names: list[str],
            ori_file_names: list[str], system_prompt: str,
            model_params: dict[str, Any]) -> Generator[str, None, None]:
        try:
            contents = []
            contents.append({"type": "text", "text": f"ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…å•é¡Œï¼š{user_input}"})
            contents.append({
                "type":
                "text",
                "text":
                "è«‹ä½ åƒ…æ ¹æ“šä¸‹æ–¹åœ–ç‰‡å…§å®¹å›ç­”ï¼Œä¸è¦å¼•å…¥é è¨­çŸ¥è­˜æˆ–æ¨è«–ã€‚ç„¡ç‰¹åˆ¥èªªæ˜è«‹ç”¨ã€Œç¹é«”ä¸­æ–‡ã€å›ç­”ã€‚"
            })

            for i, file_name in enumerate(file_names):
                contents.append({
                    "type":
                    "text",
                    "text":
                    f"è‹¥åœ–ç‰‡ä¸­æ–‡å­—ä¸æ¸…æ™°è«‹æ˜èªªã€Œç„¡æ³•è¾¨è­˜ã€ã€‚ä»¥ä¸‹åœ–ç‰‡æ˜¯{ori_file_names[i]}çš„æª”æ¡ˆå…§å®¹"
                })
                image_data_urls = self.pdf_to_image_base64_data_urls(file_name)
                for url in image_data_urls:
                    contents.append({
                        "type": "image_url",
                        "image_url": {
                            "url": url
                        }
                    })

            messages = [{
                "role": "system",
                "content": system_prompt
            }, {
                "role": "user",
                "content": contents
            }]

            yield from self.run_gptModel_stream(
                session=session,
                chat_uuid=chat_uuid,
                user=user,
                messages=messages,
                user_input=user_input,
                expert_data=expert_data,
                model_list_id=model_list_id,
                model=model_name,
                history_message_id=history_message_id,
                params=model_params)
        except Exception as e:
            logger.error(f"Error in GPTHandler generate_model_response: {e}")
            yield error_msg('ç™¼ç”ŸéŒ¯èª¤ï¼Œè‹¥æœ‰å•é¡Œè«‹è¯ç¹«ç®¡ç†å“¡ã€‚')

    def intention_analysis(self, *, user_input: str, model_list_id: int,
                           session) -> str:
        settings_row: Settings_Schema | None = select_settings_from_key(
            key="document_agent_intention_prompt", session=session)
        try:
            if not settings_row:
                intention_map: dict[str, str] = {
                    "compare_differences": "ä½¿ç”¨è€…å¸Œæœ›æ¯”è¼ƒå…©ä»½æ–‡ä»¶çš„å·®ç•°ï¼ˆä¾‹å¦‚ï¼šå¢åˆªä¿®æ”¹äº†å“ªäº›æ¢æ–‡ï¼‰ã€‚",
                    "find_similarities": "ä½¿ç”¨è€…å¸Œæœ›æ‰¾å‡ºå…©ä»½æ–‡ä»¶ä¸­å…§å®¹ç›¸ä¼¼æˆ–ç›¸åŒçš„éƒ¨åˆ†ã€‚",
                    "merge_summarize": "ä½¿ç”¨è€…å¸Œæœ›å°‡å…©ä»½æ–‡ä»¶æ•´åˆä¸¦é€²è¡Œæ‘˜è¦ã€‚",
                    "qa_with_context": "ä½¿ç”¨è€…é‡å°æ–‡ä»¶æå‡ºå•é¡Œï¼Œéœ€è¦æ ¹æ“šæ–‡ä»¶å…§å®¹å›ç­”ã€‚",
                    "validate_consistency": "ä½¿ç”¨è€…å¸Œæœ›ç¢ºèªå…©ä»½æ–‡ä»¶åœ¨é‚è¼¯æˆ–æ¢æ–‡ä¸Šæ˜¯å¦ä¸€è‡´ã€‚",
                    "other": "ç„¡æ³•æ˜ç¢ºæ­¸é¡ç‚ºä»¥ä¸Šé¡åˆ¥çš„ä»»å‹™æ™‚é¸æ“‡æ­¤é …ã€‚"
                }
            else:
                intention_map: dict[str, str] = json.loads(
                    settings_row.value.strip())
        except Exception as e:
            intention_map: dict[str, str] = {
                "compare_differences": "ä½¿ç”¨è€…å¸Œæœ›æ¯”è¼ƒå…©ä»½æ–‡ä»¶çš„å·®ç•°ï¼ˆä¾‹å¦‚ï¼šå¢åˆªä¿®æ”¹äº†å“ªäº›æ¢æ–‡ï¼‰ã€‚",
                "find_similarities": "ä½¿ç”¨è€…å¸Œæœ›æ‰¾å‡ºå…©ä»½æ–‡ä»¶ä¸­å…§å®¹ç›¸ä¼¼æˆ–ç›¸åŒçš„éƒ¨åˆ†ã€‚",
                "merge_summarize": "ä½¿ç”¨è€…å¸Œæœ›å°‡å…©ä»½æ–‡ä»¶æ•´åˆä¸¦é€²è¡Œæ‘˜è¦ã€‚",
                "qa_with_context": "ä½¿ç”¨è€…é‡å°æ–‡ä»¶æå‡ºå•é¡Œï¼Œéœ€è¦æ ¹æ“šæ–‡ä»¶å…§å®¹å›ç­”ã€‚",
                "validate_consistency": "ä½¿ç”¨è€…å¸Œæœ›ç¢ºèªå…©ä»½æ–‡ä»¶åœ¨é‚è¼¯æˆ–æ¢æ–‡ä¸Šæ˜¯å¦ä¸€è‡´ã€‚",
                "other": "ç„¡æ³•æ˜ç¢ºæ­¸é¡ç‚ºä»¥ä¸Šé¡åˆ¥çš„ä»»å‹™æ™‚é¸æ“‡æ­¤é …ã€‚"
            }
            logger.error(f"GPTHandler intention_analysis error, ex: {e}")
        meta_intention_prompt: str = f"""
            ä½ æ˜¯ä¸€å€‹æ„åœ–è¾¨è­˜åŠ©æ‰‹ï¼Œè² è²¬æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œèˆ‡æä¾›çš„æ–‡ä»¶ï¼Œåˆ¤æ–·ä½¿ç”¨è€…çš„æ“ä½œç›®çš„æ˜¯ä»€éº¼ã€‚è«‹å¾ä»¥ä¸‹é¸é …ä¸­é¸å‡ºæœ€åˆé©çš„ä¸€å€‹æ„åœ–ï¼š
            {intention_map}
            è«‹åƒ…å›å‚³ä¸€å€‹ intent keyï¼Œä¾‹å¦‚ï¼š compare_differences æˆ– otherã€‚
        """
        try:
            messages: list[ChatCompletionMessageParam] = [{
                "role":
                "system",
                "content":
                meta_intention_prompt
            }, {
                "role": "user",
                "content": user_input
            }]
            response = self.client.chat.completions.create(model="gpt-4o-mini",
                                                           messages=messages,
                                                           max_tokens=8000,
                                                           temperature=1)
            content = response.choices[0].message.content
            if not content:
                return "other"
            return content
        except Exception as e:
            logger.error(f"GPTHandler intention_analysis error, ex: {e}")
            return "other"
